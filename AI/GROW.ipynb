{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4deGLF6uV-w"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.30.2\n",
        "!pip install datasets==2.13.1\n",
        "!pip install accelerate==0.20.3\n",
        "!pip install sentencepiece==0.1.99\n",
        "!pip install umap-learn==0.5.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-quA6JNCZIS"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoM5ixSkCh4d"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_name = \"djifg/GROW-classification\"\n",
        "dataset = load_dataset(dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAKimsfGDEK-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_ckpt = \"beomi/kcbert-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVTeWJH6CwER"
      },
      "outputs": [],
      "source": [
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=True, truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oq0gy8LeDNt9"
      },
      "outputs": [],
      "source": [
        "dataset_encoded = dataset.map(tokenize, batched=True, batch_size=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imgLcc-bCwH4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "import torch\n",
        "model_ckpt = \"beomi/kcbert-large\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoModel.from_pretrained(model_ckpt).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfkVco5bCwV_"
      },
      "outputs": [],
      "source": [
        "def extract_hidden_states(batch):\n",
        "    inputs = {k:v.to(device) for k,v in batch.items()\n",
        "              if k in tokenizer.model_input_names}\n",
        "    with torch.no_grad():\n",
        "        last_hidden_state = model(**inputs).last_hidden_state\n",
        "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRaE7uKICwZi"
      },
      "outputs": [],
      "source": [
        "dataset_encoded.set_format(\"torch\",\n",
        "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTT0pqhTCwdE"
      },
      "outputs": [],
      "source": [
        "dataset_hidden = dataset_encoded.map(extract_hidden_states, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zet6nUZNCwgn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "X_train = np.array(dataset_hidden[\"train\"][\"hidden_state\"])\n",
        "X_valid = np.array(dataset_hidden[\"validation\"][\"hidden_state\"])\n",
        "y_train = np.array(dataset_hidden[\"train\"][\"label\"])\n",
        "y_valid = np.array(dataset_hidden[\"validation\"][\"label\"])\n",
        "X_train.shape, X_valid.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcNwpr6ID0rw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from umap import UMAP\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "X_scaled = MinMaxScaler().fit_transform(X_train)\n",
        "\n",
        "mapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)\n",
        "\n",
        "df_emb = pd.DataFrame(mapper.embedding_, columns=[\"X\", \"Y\"])\n",
        "df_emb[\"label\"] = y_train\n",
        "df_emb.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSMPyft3D1Yh"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "num_labels = 5\n",
        "model = (AutoModelForSequenceClassification\n",
        "         .from_pretrained(model_ckpt, num_labels=num_labels)\n",
        "         .to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8x1AXVrD1bH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdIW6I8BD1iX"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "logging_steps = len(dataset_encoded[\"train\"]) // batch_size\n",
        "model_name =  \"djifg/grow_classification_kcbert\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_name,\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    disable_tqdm=False,\n",
        "    logging_steps=logging_steps,\n",
        "    push_to_hub=True,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    log_level=\"error\",\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=dataset_encoded[\"train\"],\n",
        "    eval_dataset=dataset_encoded[\"validation\"],\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRBF2H76pGoS"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub(commit_message=\"Training completed!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
